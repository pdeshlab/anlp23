{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5792cb96",
   "metadata": {
    "id": "5792cb96"
   },
   "source": [
    "In this notebook, we'll explore the basics of token representations in BERT and use it to find token nearest neighbors.  You should open this notebook in Google Colab, or use smaller BERT models locally (as in previous notebooks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tYjrwwbFaKWW",
   "metadata": {
    "id": "tYjrwwbFaKWW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/d1/3bba59606141ae808017f6fde91453882f931957f125009417b87a281067/transformers-4.34.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.12.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/piadeshpande/anaconda3/envs/anlp/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/piadeshpande/anaconda3/envs/anlp/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/piadeshpande/anaconda3/envs/anlp/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/piadeshpande/anaconda3/envs/anlp/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/piadeshpande/anaconda3/envs/anlp/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/a1/e9/80f82aaa756e1345f80baba24af40eda58009560fa5263ff1b2a1ac32e7d/tokenizers-0.14.1-cp311-cp311-macosx_10_7_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp311-cp311-macosx_10_7_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/22/3d/32f819203115c314391268d69423eb4653e7478e8d5eaf87ae79ce434263/safetensors-0.4.0-cp311-cp311-macosx_10_7_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp311-cp311-macosx_10_7_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/piadeshpande/anaconda3/envs/anlp/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/fe/d3/e1aa96437d944fbb9cc95d0316e25583886e9cd9e6adc07baad943524eda/fsspec-2023.9.2-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/piadeshpande/anaconda3/envs/anlp/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/piadeshpande/anaconda3/envs/anlp/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/piadeshpande/anaconda3/envs/anlp/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/piadeshpande/anaconda3/envs/anlp/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/piadeshpande/anaconda3/envs/anlp/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp311-cp311-macosx_10_7_x86_64.whl (439 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.1-cp311-cp311-macosx_10_7_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.12.4 fsspec-2023.9.2 huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc388c",
   "metadata": {
    "id": "eddc388c"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a12638",
   "metadata": {
    "id": "88a12638"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f34541",
   "metadata": {
    "id": "82f34541"
   },
   "source": [
    "BERT uses WordPiece tokenization, which breaks down words that don't appear within its 30K-word vocabulary into small pieces.  The word \"vaccinated\", for instanced, is tokenized as [\"va\", \"##cci\", \"##nated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1e451",
   "metadata": {
    "id": "b2a1e451"
   },
   "outputs": [],
   "source": [
    "inputs=tokenizer(\"New data shows 26 states have fully vaccinated more than half their residents.\", return_tensors=\"pt\")\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a390bb2d",
   "metadata": {
    "id": "a390bb2d"
   },
   "outputs": [],
   "source": [
    "inputs=tokenizer(\"BERT is supercalifragilisticexpialidocious\", return_tensors=\"pt\")\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058b2be",
   "metadata": {
    "id": "b058b2be"
   },
   "source": [
    "As mentioned in class, notice how every sentence input to BERT is wrapped in two tags: a start [CLS] tag and an ending [SEP] tag.  BERT will generate representations of each WordPiece token, including these special [CLS] and [SEP] tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9c2fa",
   "metadata": {
    "id": "e4a9c2fa"
   },
   "source": [
    "To generate representations for the input tokens, simply pass the input through the BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc73aa",
   "metadata": {
    "id": "e2bc73aa"
   },
   "outputs": [],
   "source": [
    "inputs=tokenizer(\"This jam is delicious\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe3f309",
   "metadata": {
    "id": "bbe3f309"
   },
   "source": [
    "Representations for each of BERT layers (12 in this model) are accessible here, but let's explore just the outputs from the final layer.  This BERT model has 768-dimensional representations, so this 6-token input ([CLS, this, jam, is, delicious, [SEP]) has an output that is is a 1 x 6 tokens x 768 dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d8d59",
   "metadata": {
    "id": "967d8d59"
   },
   "outputs": [],
   "source": [
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe68cc0",
   "metadata": {
    "id": "6fe68cc0"
   },
   "outputs": [],
   "source": [
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888e87d",
   "metadata": {
    "id": "c888e87d"
   },
   "source": [
    "What can we do with just these representations?  While we used word2vec-style static embeddings to find nearest neighbors for word *types*, we can do the same here for word *tokens*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccbb32",
   "metadata": {
    "id": "6eccbb32"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79b4ed",
   "metadata": {
    "id": "0b79b4ed"
   },
   "outputs": [],
   "source": [
    "query=\"I ate some jam with toast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc6477e",
   "metadata": {
    "id": "cbc6477e"
   },
   "outputs": [],
   "source": [
    "comp_sents=[\"She got me out of a real jam\", \"This jam is made of strawberries\", \"I sat in a traffic jam for 2 hours\", \"The Grateful Dead used to jam for like two days straight.\", \"My grandma makes the best jam.\", \"I had to jam on the brakes to avoid hitting him.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be898d38",
   "metadata": {
    "id": "be898d38"
   },
   "outputs": [],
   "source": [
    "def get_bert_for_token(string, term):\n",
    "\n",
    "    # tokenize\n",
    "    inputs = tokenizer(string, return_tensors=\"pt\")\n",
    "\n",
    "    # convert input ids to words\n",
    "    tokens=tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "    # find the first location of the query term among those tokens (so we know which BERT rep to use)\n",
    "    term_idx=tokens.index(term)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # return the BERT rep for that token index\n",
    "    # The output is a pytorch tensor object, but let's convert it to a numpy object to work with numpy functions\n",
    "\n",
    "    return outputs.last_hidden_state[0][term_idx].detach().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719345c7",
   "metadata": {
    "id": "719345c7"
   },
   "outputs": [],
   "source": [
    "query_rep=get_bert_for_token(query, \"jam\")\n",
    "print(query_rep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0e1e8",
   "metadata": {
    "id": "ddd0e1e8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vals=[]\n",
    "for sent in comp_sents:\n",
    "    comp_rep=get_bert_for_token(sent, \"jam\")\n",
    "    cos_sim=cosine_similarity(query_rep, comp_rep)\n",
    "    vals.append((cos_sim, query, sent))\n",
    "\n",
    "for c, q, s in reversed(sorted(vals)):\n",
    "    print(\"%.3f\\t%s\\t%s\" % (c, q, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7167cc",
   "metadata": {
    "id": "af7167cc"
   },
   "source": [
    "**Q**: Brainstorm the variety of things you can do with token-level word representations like this and we'll discuss them at the end of class.  As one example, adapt the code above to find *any* word that is most similar to *jam* in \"I ate some jam with toast\" in the following sentences.  Are you able to find token-level synonyms this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9294232d",
   "metadata": {
    "id": "9294232d"
   },
   "outputs": [],
   "source": [
    "comp_sents=[\"My grandma makes the best jelly.\", \"Jelly is made of strawberries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42395734",
   "metadata": {
    "id": "42395734"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
