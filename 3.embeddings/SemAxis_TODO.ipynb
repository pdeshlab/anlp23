{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SemAxis](https://arxiv.org/pdf/1806.05521.pdf) is a method for scoring terms along a user-defined axis (e.g., positive-negative, concrete-abstract, hot-cold), which can be used for a range of empirical questions (for one example, see [Kozlowski et al. 2019](https://journals.sagepub.com/doi/full/10.1177/0003122419877135)). In this activity, you'll implement SemAxis using word representations from Glove, and use it to explore corpus-specific conceptual associations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim should be installed before running this notebook; if not, install with:\n",
    "\n",
    "`conda install gensim`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import numpy.linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity, we'll be working with pre-trained word embeddings using the `gensim` library, which provides a number of functions for accessing representations for individual words and comparing them.  The representations we'll use come from [Glove](https://nlp.stanford.edu/projects/glove/), which are trained on web data from the [Common Crawl](https://en.wikipedia.org/wiki/Common_Crawl) corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(\"../data/glove.6B.100d.100K.w2v.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_vector=glove[\"good\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.030769   0.11993    0.53909   -0.43696   -0.73937   -0.15345\n",
      "  0.081126  -0.38559   -0.68797   -0.41632   -0.13183   -0.24922\n",
      "  0.441      0.085919   0.20871   -0.063582   0.062228  -0.051234\n",
      " -0.13398    1.1418     0.036526   0.49029   -0.24567   -0.412\n",
      "  0.12349    0.41336   -0.48397   -0.54243   -0.27787   -0.26015\n",
      " -0.38485    0.78656    0.1023    -0.20712    0.40751    0.32026\n",
      " -0.51052    0.48362   -0.0099498 -0.38685    0.034975  -0.167\n",
      "  0.4237    -0.54164   -0.30323   -0.36983    0.082836  -0.52538\n",
      " -0.064531  -1.398     -0.14873   -0.35327   -0.1118     1.0912\n",
      "  0.095864  -2.8129     0.45238    0.46213    1.6012    -0.20837\n",
      " -0.27377    0.71197   -1.0754    -0.046974   0.67479   -0.065839\n",
      "  0.75824    0.39405    0.15507   -0.64719    0.32796   -0.031748\n",
      "  0.52899   -0.43886    0.67405    0.42136   -0.11981   -0.21777\n",
      " -0.29756   -0.1351     0.59898    0.46529   -0.58258   -0.02323\n",
      " -1.5442     0.01901   -0.015877   0.024499  -0.58017   -0.67659\n",
      " -0.040379  -0.44043    0.083292   0.20035   -0.75499    0.16918\n",
      " -0.26573   -0.52878    0.17584    1.065    ]\n"
     ]
    }
   ],
   "source": [
    "print(good_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions useful for this activity include the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (100,) (100,) [0.7592798]\n"
     ]
    }
   ],
   "source": [
    "# access the representation for a single word\n",
    "great_vector=glove[\"great\"]\n",
    "\n",
    "# use numpy to average multiple vector representations together\n",
    "vecs_to_average=[good_vector, great_vector]\n",
    "average=np.mean(vecs_to_average, axis=0)\n",
    "\n",
    "# calculate the cosine similariy between two vectors\n",
    "cosine_similarity=glove.cosine_similarities(good_vector, [great_vector])\n",
    "\n",
    "print(good_vector.shape, great_vector.shape, average.shape, cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the [SemAxis](https://arxiv.org/pdf/1806.05521.pdf) method as described in class. Given a set of word embeddings for positive terms $S^+ = \\{v_1^+, \\ldots v_n^+\\}$ and embeddings for negative terms $S^- = \\{v_1^-, \\ldots v_n^-\\}$ that define the endpoints of the axis, your output should be a single real-value score for an input word $w$ with word representation $v_w$:\n",
    "\n",
    "$$\n",
    "score(w)_{\\mathbf{V_\\textrm{axis}}} = \\textrm{cos}(v_w, \\mathbf{V}_\\textrm{axis})\n",
    "$$\n",
    "\n",
    "Where: \n",
    "$$\n",
    "\\mathbf{V}^+ = {1 \\over n} \\sum_1^n v_i^+\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{V}^- = {1 \\over m} \\sum_1^m v_i^-\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{V}_{\\textrm{axis}} = \\mathbf{V}^+ - \\mathbf{V}^-\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_terms = [\"good\", \"great\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semaxis_score(vectors, positive_terms=None, negative_terms=None, target_word=None):\n",
    "    \n",
    "    # access the representation of target word\n",
    "    target_vector= glove[target_word]\n",
    "    \n",
    "    pos = []\n",
    "    for pos in positive_terms:\n",
    "        pos.append(glove[pos])\n",
    "    \n",
    "    neg = []\n",
    "    for neg in negative_terms:\n",
    "        neg.append(glove[neg])\n",
    "\n",
    "    # use numpy to average multiple vector representations together\n",
    "    vecs_to_average=[pos, neg]\n",
    "    average =np.mean(vecs_to_average, axis=0)\n",
    "\n",
    "    # calculate the cosine similariy between two vectors\n",
    "    cosine_similarity=glove.cosine_similarities(good_vector, [great_vector])\n",
    "\n",
    "    score = cosine_similarity\n",
    "\n",
    "    return neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# should be 0.342\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m get_semaxis_score(glove, positive_terms\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwoman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwomen\u001b[39m\u001b[38;5;124m\"\u001b[39m], negative_terms\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmen\u001b[39m\u001b[38;5;124m\"\u001b[39m], target_word\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactress\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m, in \u001b[0;36mget_semaxis_score\u001b[0;34m(vectors, positive_terms, negative_terms, target_word)\u001b[0m\n\u001b[1;32m      6\u001b[0m pos \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m positive_terms:\n\u001b[0;32m----> 8\u001b[0m     pos\u001b[38;5;241m.\u001b[39mappend(glove[pos])\n\u001b[1;32m     10\u001b[0m neg \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m neg \u001b[38;5;129;01min\u001b[39;00m negative_terms:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# should be 0.342\n",
    "get_semaxis_score(glove, positive_terms=[\"woman\", \"women\"], negative_terms=[\"man\", \"men\"], target_word=\"actress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's score a set of target terms along that axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_list_of_targets(vectors, positive_terms=None, negative_terms=None, target_words=None):\n",
    "    scores=[]\n",
    "    for target in target_words:\n",
    "        scores.append((get_semaxis_score(vectors, positive_terms, negative_terms, target), target))\n",
    "\n",
    "    for k,v in reversed(sorted(scores)):\n",
    "        print(\"%.3f\\t%s\" % (k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=[\"doctor\", \"nurse\", \"actor\", \"actress\", \"mechanic\", \"librarian\", \"architect\", \"magician\", \"cook\", \"chef\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list_of_targets(glove, positive_terms=[\"woman\", \"women\"], negative_terms=[\"man\", \"men\"], target_words=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define **your own concept axis** by selecting a set of positive and negative terms and illustrate its utility by scoring a set of 10 target terms (as we did above).  If you've implemented  `get_semaxis_score` above, you only need to add terms to the `positive_terms` and `negative_terms` lists below and execute this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_terms=[]\n",
    "negative_terms=[]\n",
    "targets=[]\n",
    "\n",
    "score_list_of_targets(glove, positive_terms=positive_terms, negative_terms=negative_terms, target_words=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume now that you're able to score all words in a vocabulary along several conceptual dimensions (like the one you've defined) for a given set of word embeddings trained on a dataset.  What could you do with that score? Brainstorm possible applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
