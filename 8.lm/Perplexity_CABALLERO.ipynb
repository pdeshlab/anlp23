{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c35ba5",
   "metadata": {},
   "source": [
    "In this homework, you'll explore perplexity as a measure of evaluation for language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b52b3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33eb376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    sequences=[]\n",
    "    with open(filename) as file:\n",
    "        data=file.read()\n",
    "        sents=sent_tokenize(data)\n",
    "        for sent in sents:\n",
    "            tokens=word_tokenize(sent)\n",
    "            sequences.append(tokens)\n",
    "            \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feeb46f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file and tokenize them into sequences comprised of tokens.\n",
    "\n",
    "# Pride and Prejudice (Jane Austen)\n",
    "sequences=read_file(\"../data/stylometry/1342_pride_and_prejudice.txt\")\n",
    "\n",
    "max_sequences=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "214dbd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramModel():\n",
    "\n",
    "    def __init__(self, sequences, order):\n",
    "        \n",
    "        # For this exercise we're going to encode the LM as a sparse dictionary (training less storage for more compute)\n",
    "        # We'll store the LM as a dictionary with the conditioning context as keys; each value is a \n",
    "        # Counter object that keeps track of the number of times we see a word following that context.\n",
    "        \n",
    "        self.counts={}\n",
    "        \n",
    "        # Markov order (order 1 = conditioning on previous 1 word; order 2 = previous 2 words, etc.)\n",
    "        self.order=order\n",
    "        \n",
    "        vocab={\"[END]\":0}\n",
    "                \n",
    "        for s_idx, tokens in enumerate(sequences):\n",
    "            # We'll add [START] and [END] tokens to encode the beginning/end of sentences\n",
    "            token_copy=copy.deepcopy(tokens)\n",
    "            for i in range(order):\n",
    "                token_copy.insert(0, \"[START]\")\n",
    "            token_copy.append(\"[END]\")\n",
    "            \n",
    "        \n",
    "            for i in range(order, len(token_copy)):\n",
    "                context=\" \".join(token_copy[i-order:i])\n",
    "                word=token_copy[i]\n",
    "                \n",
    "                if word not in vocab:\n",
    "                    vocab[word]=len(vocab)\n",
    "                \n",
    "                # For just the first sentence, print the conditioning context + word\n",
    "                if s_idx == 0:\n",
    "                    print(\"Context: %s Next: %s\" % (context.ljust(50), word))\n",
    "                    \n",
    "                if context not in self.counts:\n",
    "                    self.counts[context]=Counter()\n",
    "                self.counts[context][word]+=1\n",
    "                \n",
    "\n",
    "\n",
    "    def sample(self, context):\n",
    "\n",
    "        total=sum(self.counts[context].values())\n",
    "        \n",
    "        dist=[]\n",
    "        vocab=[]\n",
    "\n",
    "        # Create a probability distribution for each conditioning context, over the vocab that we've observed it with.\n",
    "        for idx, word in enumerate(self.counts[context]):\n",
    "            prob=self.counts[context][word]/total\n",
    "            dist.append(prob)\n",
    "            vocab.append(word)\n",
    "\n",
    "        index=np.argmax(np.random.multinomial(1, pvals=dist))\n",
    "        return vocab[index]\n",
    "        \n",
    "    def generate_sequence(self):\n",
    "        generated=[\"[START]\"]*(self.order)\n",
    "        word=None\n",
    "        while word != \"[END]\":\n",
    "            context=' '.join(generated[-self.order:] if self.order > 0 else \"\")\n",
    "            word=self.sample(context)\n",
    "            print(word)\n",
    "            generated.append(word)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384b034",
   "metadata": {},
   "source": [
    "Let's create some language models of different orders from *Pride and Prejudice*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4502623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:                                                    Next: Chapter\n",
      "Context:                                                    Next: 1\n",
      "Context:                                                    Next: It\n",
      "Context:                                                    Next: is\n",
      "Context:                                                    Next: a\n",
      "Context:                                                    Next: truth\n",
      "Context:                                                    Next: universally\n",
      "Context:                                                    Next: acknowledged\n",
      "Context:                                                    Next: ,\n",
      "Context:                                                    Next: that\n",
      "Context:                                                    Next: a\n",
      "Context:                                                    Next: single\n",
      "Context:                                                    Next: man\n",
      "Context:                                                    Next: in\n",
      "Context:                                                    Next: possession\n",
      "Context:                                                    Next: of\n",
      "Context:                                                    Next: a\n",
      "Context:                                                    Next: good\n",
      "Context:                                                    Next: fortune\n",
      "Context:                                                    Next: ,\n",
      "Context:                                                    Next: must\n",
      "Context:                                                    Next: be\n",
      "Context:                                                    Next: in\n",
      "Context:                                                    Next: want\n",
      "Context:                                                    Next: of\n",
      "Context:                                                    Next: a\n",
      "Context:                                                    Next: wife\n",
      "Context:                                                    Next: .\n",
      "Context:                                                    Next: [END]\n"
     ]
    }
   ],
   "source": [
    "ngram0=NgramModel(sequences[:max_sequences], order=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a51b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [START]                                            Next: Chapter\n",
      "Context: Chapter                                            Next: 1\n",
      "Context: 1                                                  Next: It\n",
      "Context: It                                                 Next: is\n",
      "Context: is                                                 Next: a\n",
      "Context: a                                                  Next: truth\n",
      "Context: truth                                              Next: universally\n",
      "Context: universally                                        Next: acknowledged\n",
      "Context: acknowledged                                       Next: ,\n",
      "Context: ,                                                  Next: that\n",
      "Context: that                                               Next: a\n",
      "Context: a                                                  Next: single\n",
      "Context: single                                             Next: man\n",
      "Context: man                                                Next: in\n",
      "Context: in                                                 Next: possession\n",
      "Context: possession                                         Next: of\n",
      "Context: of                                                 Next: a\n",
      "Context: a                                                  Next: good\n",
      "Context: good                                               Next: fortune\n",
      "Context: fortune                                            Next: ,\n",
      "Context: ,                                                  Next: must\n",
      "Context: must                                               Next: be\n",
      "Context: be                                                 Next: in\n",
      "Context: in                                                 Next: want\n",
      "Context: want                                               Next: of\n",
      "Context: of                                                 Next: a\n",
      "Context: a                                                  Next: wife\n",
      "Context: wife                                               Next: .\n",
      "Context: .                                                  Next: [END]\n"
     ]
    }
   ],
   "source": [
    "ngram1=NgramModel(sequences[:max_sequences], order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35ee1ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [START] [START]                                    Next: Chapter\n",
      "Context: [START] Chapter                                    Next: 1\n",
      "Context: Chapter 1                                          Next: It\n",
      "Context: 1 It                                               Next: is\n",
      "Context: It is                                              Next: a\n",
      "Context: is a                                               Next: truth\n",
      "Context: a truth                                            Next: universally\n",
      "Context: truth universally                                  Next: acknowledged\n",
      "Context: universally acknowledged                           Next: ,\n",
      "Context: acknowledged ,                                     Next: that\n",
      "Context: , that                                             Next: a\n",
      "Context: that a                                             Next: single\n",
      "Context: a single                                           Next: man\n",
      "Context: single man                                         Next: in\n",
      "Context: man in                                             Next: possession\n",
      "Context: in possession                                      Next: of\n",
      "Context: possession of                                      Next: a\n",
      "Context: of a                                               Next: good\n",
      "Context: a good                                             Next: fortune\n",
      "Context: good fortune                                       Next: ,\n",
      "Context: fortune ,                                          Next: must\n",
      "Context: , must                                             Next: be\n",
      "Context: must be                                            Next: in\n",
      "Context: be in                                              Next: want\n",
      "Context: in want                                            Next: of\n",
      "Context: want of                                            Next: a\n",
      "Context: of a                                               Next: wife\n",
      "Context: a wife                                             Next: .\n",
      "Context: wife .                                             Next: [END]\n"
     ]
    }
   ],
   "source": [
    "ngram2=NgramModel(sequences[:max_sequences], order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55ac8d",
   "metadata": {},
   "source": [
    "**Q1.** Create a `perplexity` function that can takes two arguments: a.) a model of *any* ngram order (from the class above); and b.) a sequence to calculate perplexity for.  You'll recall from class that perplexity under a particular language model for sequence $w$ is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\textrm{perplexity}_{model}(w) = \\exp\\left(-{1 \\over N} \\sum_{i=1}^N \\log P_{model}(w_i) \\right)\n",
    "$$\n",
    "\n",
    "$P_{model}(w_i)$ calculates the probability of token $w_i$ using whatever assumptions that model makes -- for a bigram model (order 1), this is $P(w_i \\mid w_{i-1})$, for a trigram model (order 2), this is $P(w_i \\mid w_{i-2}, w_{i-1})$, etc.  Two things to note:\n",
    "\n",
    "* When calculating the probability of the first word(s), be sure to get the conditioning context right.  The conditioning context for the first word in a trigram model, for example, is $P(w_i \\mid$ [START] [START]$)$.\n",
    "* Perplexity is only calculated for the words in the actual sequence.  We don't include the $P$([START]) or $P$([END]) in the perplexity calculuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1d32301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, tokens):\n",
    "    total_log_prob = 0 \n",
    "    total_tokens = 0  \n",
    "\n",
    "    for i in range(model.order, len(tokens)):\n",
    "        # Iterate through the tokens, starting from the 'order' index (context size) to exclude [START].\n",
    "        context = ' '.join(tokens[i - model.order:i])  # Extract the context based on the model's order.\n",
    "        current_token = tokens[i]  # Get the current token.\n",
    "\n",
    "        if context in model.counts:\n",
    "            # Calculate the probability of the current token given the context.\n",
    "            context_counts = model.counts[context]\n",
    "            token_count = context_counts.get(current_token, 0)\n",
    "\n",
    "            if token_count > 0:\n",
    "                # Avoid division by zero and calculate the probability.\n",
    "                token_prob = token_count / sum(context_counts.values())\n",
    "\n",
    "                # Add the logarithm of the probability to the total log probability.\n",
    "                total_log_prob -= np.log(token_prob)\n",
    "                total_tokens += 1  # Increment the token count.\n",
    "\n",
    "    # Calculate perplexity as the exponential of the average log probability.\n",
    "    perplexity = np.exp(total_log_prob / total_tokens)\n",
    "\n",
    "    return (perplexity)  # Return the perplexity value.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d0a8a",
   "metadata": {},
   "source": [
    "**Q2**. Execute that perplexity function for the following language models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2a4b0e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202.22631978521557"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(ngram0, word_tokenize(\"She was a great friend of Mr. Bingley.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd2d4b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.554084030184423"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(ngram1, word_tokenize(\"She was a great friend of Mr. Bingley.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05be627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.521370397120716"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(ngram2, word_tokenize(\"She was a great friend of Mr. Bingley.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a331078",
   "metadata": {},
   "source": [
    "**Q3.** What is the perplexity of \"She was a really great friend of Mr. Bingley.\" in the trigram language model trained above?  Explain in 100 words that behavior is expected (and correct) given how this language model and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0a4f81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(ngram2, word_tokenize(\"She was a really great friend of Mr. Bingley.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee04e74",
   "metadata": {},
   "source": [
    "**answer:** The perplexity score of this sentence is ([INSERT]). This makes sense because 1) as the markov order assumption increases, perp goes down because the model is better fit to the data. 2)This is correct because we are using the entire corpus?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
