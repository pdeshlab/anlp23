{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8780af9",
   "metadata": {},
   "source": [
    "In \"[Literary Pattern Recognition](https://www.journals.uchicago.edu/doi/full/10.1086/684353)\", Long and So train a classifier to differentiate haiku poems from non-haiku poems, and find that many features help do so.  In class, we've discussed the importance of representation--how you *describe* a text computationally influences the kinds of things you are able to do with it.  While Long and So explore description in the context of classification, in this homework, you'll see how well you can design features that can differentiate these two classes *without* any supervision. Are you able to featurize a collection of poems such that two clusters (haiku/non-haiku) emerge when using KMeans clustering, with the text representation as your only degree of freedom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d30ae833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv, os, re\n",
    "import nltk\n",
    "from scipy import sparse\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f517024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_texts(path, metadata, filepath_col):\n",
    "    data=[]\n",
    "    with open(metadata, encoding=\"utf-8\") as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        next(csv_reader)\n",
    "        for cols in csv_reader:\n",
    "            poem_path=os.path.join(path, cols[filepath_col])\n",
    "            if os.path.exists(poem_path):\n",
    "                with open(poem_path, encoding=\"utf-8\") as poem_file:\n",
    "                    poem=poem_file.read()\n",
    "                    data.append(poem)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81026d2",
   "metadata": {},
   "source": [
    "Here we'll use data originally released on Github to support \"Literary Pattern Recognition\": [https://github.com/hoytlong/PatternRecognition](https://github.com/hoytlong/PatternRecognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfbe9556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "haiku=read_texts(\"../data/haiku/long_so_haiku\", \"../data/haiku/Haikus.csv\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "336544fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "others=read_texts(\"../data/haiku/long_so_others\", \"../data/haiku/OthersData.csv\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2b4d33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# don't change anything within this code block\n",
    "\n",
    "def run_all(haiku, others, feature_function):\n",
    "    \n",
    "    X, Y, featurize_vocab=feature_function(haiku, others)\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "    nmi=metrics.normalized_mutual_info_score(Y, kmeans.labels_)\n",
    "    print(\"%.3f NMI\" % nmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c16b1",
   "metadata": {},
   "source": [
    "As one example, let's take a simple featurization and represent each poem by a binary indicator of the dictionary word types it contains.  \"To be or not to be\", for example, would be represented as {\"to\": 1, \"be\": 1, \"or\": 1, \"not\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f433360a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function takes in a list of haiku poems and non-haiku poems, and returns:\n",
    "\n",
    "# X (sparse matrix, with poems as rows and features as columns)\n",
    "# Y (list of poem labels, with 1=haiku and 0=non-haiku)\n",
    "# feature_vocab (dict mapping feature name to feature ID)\n",
    "\n",
    "def unigram_featurize_all(haiku, others):\n",
    "\n",
    "    def unigram_featurize(poem, feature_vocab):\n",
    "        \n",
    "        # featurize text by just noting the binary presence of words within it\n",
    "        \n",
    "        feats={}\n",
    "\n",
    "        tokens=nltk.word_tokenize(poem.lower())\n",
    "        for token in tokens:\n",
    "            if token not in feature_vocab:\n",
    "                feature_vocab[token]=len(feature_vocab)\n",
    "            feats[feature_vocab[token]]=1\n",
    "        return feats\n",
    "\n",
    "    feature_vocab={}\n",
    "    data=[]\n",
    "    Y=[]\n",
    "\n",
    "    for poem in haiku:\n",
    "        feats=unigram_featurize(poem, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(1)\n",
    "    for poem in others:\n",
    "        feats=unigram_featurize(poem, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(0)\n",
    "    \n",
    "    # since the data above has all haiku ordered before non-haiku, let's shuffle them\n",
    "    temp = list(zip(data, Y))\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "\n",
    "    # we'll use a sparse representation since our features are sparse\n",
    "    X=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "\n",
    "    for idx,feats in enumerate(data):\n",
    "        for f in feats:\n",
    "            X[idx,f]=feats[f]\n",
    "    \n",
    "    return X, Y, feature_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695d6fb",
   "metadata": {},
   "source": [
    "This method yields an NMI of ~0.07 (with some variability due to the randomness of KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b420ead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.073 NMI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piadeshpande/anaconda3/envs/anlp/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "run_all(haiku, others, unigram_featurize_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e57de6",
   "metadata": {},
   "source": [
    "**Q1**: Copy the `unigram_featurize_all` code above and adapt it to create your own featurization method named `fancy_featurize_all`.  You may use whatever information you like to represent these poems for the purposes of clustering them into two categories, but you must use the KMeans clustering (with 2 clusters) as defined in `run_all`.  Use your own understanding of haiku, or read the Long and So article above for other ideas.  Are you able to improve over an NMI of 0.07?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19b47e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fancy_featurize_all(haiku, others):\n",
    "    \n",
    "    def syllable_count(poem):\n",
    "        feats={}\n",
    "        tokens=nltk.word_tokenize(poem.lower())\n",
    "        for token in tokens:\n",
    "           \n",
    "            count = 0\n",
    "            vowels = \"aeiouy\"\n",
    "            if token[0] in vowels:\n",
    "                count += 1\n",
    "            for index in range(1, len(token)):\n",
    "                if token[index] in vowels and token[index - 1] not in vowels:\n",
    "                    count += 1\n",
    "            if token.endswith(\"e\"):\n",
    "                count -= 1\n",
    "            if count == 0:\n",
    "                count += 1\n",
    "            return count\n",
    "\n",
    "    feature_vocab={}\n",
    "    data=[]\n",
    "    Y=[]\n",
    "\n",
    "    for poem in haiku:\n",
    "        feats=syllable_count(poem)\n",
    "        data.append(feats)\n",
    "        Y.append(1)\n",
    "    for poem in others:\n",
    "        feats=syllable_count(poem)\n",
    "        data.append(feats)\n",
    "        Y.append(0)\n",
    "    \n",
    "    # since the data above has all haiku ordered before non-haiku, let's shuffle them\n",
    "    temp = list(zip(data, Y))\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "\n",
    "    # we'll use a sparse representation since our features are sparse\n",
    "    X=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "\n",
    "    for idx,feats in enumerate(data):\n",
    "        for f in feats:\n",
    "            X[idx,f]=feats[f]\n",
    "    \n",
    "    return X, Y, feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a06afb26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_all(haiku, others, fancy_featurize_all)\n",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m, in \u001b[0;36mrun_all\u001b[0;34m(haiku, others, feature_function)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_all\u001b[39m(haiku, others, feature_function):\n\u001b[0;32m----> 5\u001b[0m     X, Y, featurize_vocab\u001b[38;5;241m=\u001b[39mfeature_function(haiku, others)\n\u001b[1;32m      6\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m      7\u001b[0m     nmi\u001b[38;5;241m=\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mnormalized_mutual_info_score(Y, kmeans\u001b[38;5;241m.\u001b[39mlabels_)\n",
      "Cell \u001b[0;32mIn[41], line 43\u001b[0m, in \u001b[0;36mfancy_featurize_all\u001b[0;34m(haiku, others)\u001b[0m\n\u001b[1;32m     40\u001b[0m X\u001b[38;5;241m=\u001b[39msparse\u001b[38;5;241m.\u001b[39mlil_matrix((\u001b[38;5;28mlen\u001b[39m(data), \u001b[38;5;28mlen\u001b[39m(feature_vocab)))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,feats \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m feats:\n\u001b[1;32m     44\u001b[0m         X[idx,f]\u001b[38;5;241m=\u001b[39mfeats[f]\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y, feature_vocab\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "run_all(haiku, others, fancy_featurize_all)\n",
    "# I am getting an error here that I can't figure out related to a poem not being iterable? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feda2b4",
   "metadata": {},
   "source": [
    "**Q2**: Describe your method for featurization in 100 words and why you expect it to be able to separate haiku poems from non-haiku poems in this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4977284",
   "metadata": {},
   "source": [
    "My method for featurization counts the number of syllables in each poem. I believe this will separate haiku poems from non-haiku poems because haikus (normally) have 17 syllables (5+7+5) while non-haiku poems can have any number of syllables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4ffea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6935d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
